{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-step pipeline example\r\n",
    "\r\n",
    "In this example, we'll be building a three step pipeline which passes data from the a first step (prepare) to the second step (train) and then register the model (register).\r\n",
    "\r\n",
    "**Note:** This example requires that you've ran the notebook from the first tutorial, so that the dataset, environment, and compute cluster are set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import azureml.core\r\n",
    "from azureml.core import Workspace, Experiment, Dataset, RunConfiguration, Environment\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "\r\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will connect to the workspace. The command `Workspace.from_config()` will either:\n",
    "* Read the local `config.json` with the workspace reference (given it is there) or\n",
    "* Use the `az` CLI to connect to the workspace and use the workspace attached to via `az ml folder attach -g <resource group> -w <workspace name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(f'WS name: {ws.name}\\nRegion: {ws.location}\\nSubscription id: {ws.subscription_id}\\nResource group: {ws.resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's reference our training dataset from the last tutorial, so that we can use it as the pipeline input for the prepare step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our dataset as the default dataset (if user does not set the parameter during pipeline invocation)\n",
    "default_training_dataset = Dataset.get_by_name(ws, \"german-credit-train-tutorial\")\n",
    "\n",
    "# Parametrize dataset input to the pipeline\n",
    "training_dataset_parameter = PipelineParameter(name=\"training_dataset\", default_value=default_training_dataset)\n",
    "training_dataset_consumption = DatasetConsumptionConfig(\"training_dataset\", training_dataset_parameter).as_download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a `PipelineData` placeholder which will be used to persist and pipe data from the prepare step to the train step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = ws.get_default_datastore()\r\n",
    "prepared_data = PipelineData(\"prepared_data\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create our three-stepped pipeline that runs some preprocessing on the data and then pipes the output to the training code. The dependency graph is automatically resolved through the data input/outputs, which means we need to tell AML that registration should happen last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconfig = RunConfiguration()\r\n",
    "runconfig.environment = Environment.get(workspace=ws, name='workshop-env')\r\n",
    "\r\n",
    "prepare_step = PythonScriptStep(name=\"prepare-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='prepare.py',\r\n",
    "                        arguments=['--data-input-path', training_dataset_consumption,\r\n",
    "                                   '--data-output-path', prepared_data],\r\n",
    "                        inputs=[training_dataset_consumption],\r\n",
    "                        outputs=[prepared_data],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "train_step = PythonScriptStep(name=\"train-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='train.py',\r\n",
    "                        arguments=['--data-path', prepared_data],\r\n",
    "                        inputs=[prepared_data],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "register_step = PythonScriptStep(name=\"register-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='register.py',\r\n",
    "                        arguments=['--model_name', 'workshop-model', '--model_path', 'outputs/model.pkl'],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "register_step.run_after(train_step) # Required, as there is no implicit data dependency between the train and register steps\r\n",
    "steps = [prepare_step, train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our pipeline object and validate it. This will check the input and outputs are properly linked and that the pipeline graph is a non-cyclic graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can submit the pipeline against an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'prepare-training-pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also publish the pipeline as a RESTful API Endpoint. In this case, you can specify the dataset upon invocation of the pipeline. This is nicely possible in the `Studio UI`, goto `Endpoints`, then `Pipeline Endpoints` and then select the pipeline. Once you hit the submit button, you can select the Dataset at the bottom of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish('prepare-training-pipeline')\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using just a path as input (instead of a dataset)\r\n",
    "\r\n",
    "What if we want to run the pipeline without using a dataset, but rather just a path on the Datastore? This might make it easier to use the pipeline for e.g., batch scoring, as it removes the requirement for dataset registration. For this we can use Datapaths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\r\n",
    "\r\n",
    "default_datastore = ws.get_default_datastore()\r\n",
    "\r\n",
    "# Define default Datapath and make it configurable via PipelineParameter\r\n",
    "data_path = DataPath(datastore=default_datastore, path_on_datastore='german-credit-train-tutorial/')\r\n",
    "datapath_parameter = PipelineParameter(name=\"training_data_path\", default_value=data_path)\r\n",
    "datapath_input = (datapath_parameter, DataPathComputeBinding(mode='download'))\r\n",
    "\r\n",
    "# Same as in example above\r\n",
    "prepared_data = PipelineData(\"prepared_data\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline stays the same, expect that we swap out the data input for the first step and set it to the datapath:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconfig = RunConfiguration()\r\n",
    "runconfig.environment = Environment.get(workspace=ws, name='workshop-env')\r\n",
    "\r\n",
    "prepare_step = PythonScriptStep(name=\"prepare-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='prepare.py',\r\n",
    "                        arguments=['--data-input-path', datapath_input,\r\n",
    "                                   '--data-output-path', prepared_data],\r\n",
    "                        inputs=[datapath_input],\r\n",
    "                        outputs=[prepared_data],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "train_step = PythonScriptStep(name=\"train-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='train.py',\r\n",
    "                        arguments=['--data-path', prepared_data],\r\n",
    "                        inputs=[prepared_data],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "register_step = PythonScriptStep(name=\"register-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='register.py',\r\n",
    "                        arguments=['--model_name', 'workshop-model', '--model_path', 'outputs/model.pkl'],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "register_step.run_after(train_step) # Required, as there is no implicit data dependency between the train and register steps\r\n",
    "steps = [prepare_step, train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can validate, try it out and publish it (same as before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\r\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'prepare-training-pipeline-datapath').submit(pipeline)\r\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish('prepare-training-pipeline-datapath')\r\n",
    "published_pipeline"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c289faa3097d4ac9289519def538503f3010d283412eb21807c4abc0fc245ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('azureml11': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}