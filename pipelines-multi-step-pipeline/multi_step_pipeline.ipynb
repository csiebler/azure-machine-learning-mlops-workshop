{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-step pipeline example\r\n",
    "\r\n",
    "In this example, we'll be building a three step pipeline which passes data from the a first step (prepare) to the second step (train) and then register the model (register).\r\n",
    "\r\n",
    "**Note:** This example requires that you've ran the notebook from the first tutorial, so that the dataset, environment, and compute cluster are set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import azureml.core\r\n",
    "from azureml.core import Workspace, Experiment, Dataset, RunConfiguration, Environment\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "\r\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will connect to the workspace. The command `Workspace.from_config()` will either:\n",
    "* Read the local `config.json` with the workspace reference (given it is there) or\n",
    "* Use the `az` CLI to connect to the workspace and use the workspace attached to via `az ml folder attach -g <resource group> -w <workspace name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(f'WS name: {ws.name}\\nRegion: {ws.location}\\nSubscription id: {ws.subscription_id}\\nResource group: {ws.resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's reference our training dataset from the last tutorial, so that we can use it as the pipeline input for the prepare step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our dataset as the default dataset (if user does not set the parameter during pipeline invocation)\n",
    "default_training_dataset = Dataset.get_by_name(ws, \"german-credit-train-tutorial\")\n",
    "\n",
    "# Parametrize dataset input to the pipeline\n",
    "training_dataset_parameter = PipelineParameter(name=\"training_dataset\", default_value=default_training_dataset)\n",
    "training_dataset_consumption = DatasetConsumptionConfig(\"training_dataset\", training_dataset_parameter).as_download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a `PipelineData` placeholder which will be used to persist and pipe data from the prepare step to the train step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_datastore = ws.get_default_datastore()\r\n",
    "prepared_data = PipelineData(\"prepared_data\", datastore=default_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create our three-stepped pipeline that runs some preprocessing on the data and then pipes the output to the training code. The dependency graph is automatically resolved through the data input/outputs, which means we need to tell AML that registration should happen last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconfig = RunConfiguration()\r\n",
    "runconfig.environment = Environment.get(workspace=ws, name='workshop-env')\r\n",
    "\r\n",
    "prepare_step = PythonScriptStep(name=\"prepare-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='prepare.py',\r\n",
    "                        arguments=['--data-input-path', training_dataset_consumption,\r\n",
    "                                   '--data-output-path', prepared_data],\r\n",
    "                        inputs=[training_dataset_consumption],\r\n",
    "                        outputs=[prepared_data],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "train_step = PythonScriptStep(name=\"train-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='train.py',\r\n",
    "                        arguments=['--data-path', prepared_data],\r\n",
    "                        inputs=[prepared_data],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "register_step = PythonScriptStep(name=\"register-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name='register.py',\r\n",
    "                        arguments=['--model_name', 'workshop-model', '--model_path', 'outputs/model.pkl'],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "register_step.run_after(train_step) # Required, as there is no implicit data dependency between the train and register steps\r\n",
    "steps = [prepare_step, train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our pipeline object and validate it. This will check the input and outputs are properly linked and that the pipeline graph is a non-cyclic graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can submit the pipeline against an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'prepare-training-pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also publish the pipeline as a RESTful API Endpoint. In this case, you can specify the dataset upon invocation of the pipeline. This is nicely possible in the `Studio UI`, goto `Endpoints`, then `Pipeline Endpoints` and then select the pipeline. Once you hit the submit button, you can select the Dataset at the bottom of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish('prepare-training-pipeline')\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54b76a1167e0a2b6a6b8c7f2df323eb2ecfae9d2bbefe58fb0609bf9141d6860"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('azureml': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}