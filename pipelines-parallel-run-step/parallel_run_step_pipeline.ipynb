{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Batch Scoring pipeline example\n",
    "\n",
    "In this example, we'll build a pipeline that is able to batch score data in parallel on one or multiple nodes. This can be used to either score large amounts of data or train many models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Dataset, RunConfiguration\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will connect to the workspace. The command `Workspace.from_config()` will either:\n",
    "* Read the local `config.json` with the workspace reference (given it is there) or\n",
    "* Use the `az` CLI to connect to the workspace and use the workspace attached to via `az ml folder attach -g <resource group> -w <workspace name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(f'WS name: {ws.name}\\nRegion: {ws.location}\\nSubscription id: {ws.subscription_id}\\nResource group: {ws.resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "Let's register the provided `model.pkl` as model in our workspace. We'll use this model for batch scoring in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "Model.register(model_path=\"model.pkl\",\n",
    "               model_name=\"credit_model_tutorial\",\n",
    "               description=\"Example model for batch scoring tutorial\",\n",
    "               workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also register a dataset with data that we want to use for batch scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir='../data-batch-scoring', target_path='german-credit-batch-tutorial', overwrite=True)\n",
    "ds = Dataset.File.from_files(path=[(datastore, 'german-credit-batch-tutorial')])\n",
    "ds.register(ws, name='german-credit-batch-tutorial', description='Dataset for batch scoring tutorial', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's reference our newly created batch scoring dataset, so that we can use it as the pipeline input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataset = Dataset.get_by_name(ws, \"german-credit-batch-tutorial\")\n",
    "batch_dataset_consumption = DatasetConsumptionConfig(\"batch_dataset\", batch_dataset).as_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a output dataset that will contain our predictions. This gives us complete freedom where we want to store the predictions on the datastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_dataset = PipelineData(name='batch_output', datastore=ws.get_default_datastore()).as_dataset()\n",
    "#output_dataset = output_dataset.register(name='batch-scoring-results', create_new_version=True)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# This will put the output results into a pre-defined folder on our datastore and optionally register it as a dataset (not required)\n",
    "output_dataset = OutputFileDatasetConfig(name='batch_results',\n",
    "                                         destination=(datastore, 'batch-scoring-results/{run-id}')).register_on_complete(name='batch-scoring-results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a `ParallelRunStep` that runs our batch scoring code in parallel on one or more nodes. In this case, we use a `ParallelRunConfig` from a YAML file, that defines our batch scoring job (source script, environement, parallelization, target cluster, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_run_config = ParallelRunConfig.load_yaml(workspace=ws, path=\"parallel_runconfig.yml\")\n",
    "\n",
    "batch_step = ParallelRunStep(\n",
    "    name=\"batch-inference-step\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    arguments=['--model_name', 'credit_model_tutorial'],\n",
    "    inputs=[batch_dataset_consumption],\n",
    "    side_inputs=[],\n",
    "    output=output_dataset,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "steps = [batch_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our pipeline object and validate it. This will check the input and outputs are properly linked and that the pipeline graph is a non-cyclic graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can submit the pipeline against an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'batch-scoring-pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we can nnow download the resulting dataset and have a look at our predictions. For easy of use, we'll just download it here to a folder named `temp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "Dataset.get_by_name(ws, \"batch-scoring-results\").download(target_path=\"temp/\", overwrite=True)\n",
    "with open('temp/batch-predictions.txt','r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('azureml': conda)",
   "display_name": "Python 3.7.9 64-bit ('azureml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "54b76a1167e0a2b6a6b8c7f2df323eb2ecfae9d2bbefe58fb0609bf9141d6860"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}