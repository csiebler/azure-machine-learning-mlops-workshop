{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-step pipeline examples\n",
    "\n",
    "In this example, we'll build a very simple pipeline that just contains a single train step. The dataset and compute cluster created in this tutorial will be re-used in the subsequent examples in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install azureml-sdk --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import azureml.core\r\n",
    "from azureml.core import Workspace, Experiment, Dataset, RunConfiguration, Environment\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "\r\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will connect to the workspace. The command `Workspace.from_config()` will either:\n",
    "* Read the local `config.json` with the workspace reference (given it is there) or\n",
    "* Use the `az` CLI to connect to the workspace and use the workspace attached to via `az ml folder attach -g <resource group> -w <workspace name>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(f'WS name: {ws.name}\\nRegion: {ws.location}\\nSubscription id: {ws.subscription_id}\\nResource group: {ws.resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\r\n",
    "\r\n",
    "Let's quickly a create a compute cluster named `cpu-cluster`, in case it does not exist. This is where our pipeline will run on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "except ComputeTargetException:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", min_nodes = 0, max_nodes = 1,\n",
    "                                                   idle_seconds_before_scaledown=3600)\n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an AzureML Environment. This will hold our dependencies we'll need to execute our code inside the pipeline. We'll re-use the same Environment throughout most of the samples in this repo. Typically, you might use different Environments throughout the lifecycle (e.g., one for training and one for inferencing), but for sake of simplicity, we'll keep it down to one here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_conda_specification(name='workshop-env', file_path='conda.yml')\r\n",
    "env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly build the environment - this will create a new Docker image in Azure Container Registry (ACR), and directly 'bake in' our dependencies from the conda definition. When we later use the Environment, all AML will need to do is pull the image for environment, thus saving the time for potentially a long-running conda environment creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "build = env.build(workspace=ws)\r\n",
    "build.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we'll create a new dataset and register it to the workspace. We'll be using this dataset also in the subsequent pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir='../data-training', target_path='german-credit-train-tutorial', overwrite=True)\n",
    "ds = Dataset.File.from_files(path=[(datastore, 'german-credit-train-tutorial')])\n",
    "ds.register(ws, name='german-credit-train-tutorial', description='Dataset for workshop tutorials', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's reference our newly created training dataset, so that we can use it as the pipeline input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Dataset.get_by_name(ws, \"german-credit-train-tutorial\")\n",
    "# Download dataset to compute node - we can also use .as_mount() if the dataset does not fit the machine\n",
    "training_dataset_consumption = DatasetConsumptionConfig(\"training_dataset\", training_dataset).as_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can create a `PythonScriptStep` that runs our training code, referencing our code, passing in the args, and using the environment definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runconfig = RunConfiguration()\r\n",
    "runconfig.environment = Environment.get(workspace=ws, name='workshop-env')\r\n",
    "\r\n",
    "train_step = PythonScriptStep(name=\"train-step\",\r\n",
    "                        source_directory=\"./\",\r\n",
    "                        script_name=\"train.py\",\r\n",
    "                        arguments=['--data-path', training_dataset_consumption],\r\n",
    "                        inputs=[training_dataset_consumption],\r\n",
    "                        runconfig=runconfig,\r\n",
    "                        compute_target='cpu-cluster',\r\n",
    "                        allow_reuse=False)\r\n",
    "\r\n",
    "steps = [train_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our pipeline object and validate it. This will check the input and outputs are properly linked and that the pipeline graph is a non-cyclic graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can submit the pipeline against an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, 'training-pipeline').submit(pipeline)\n",
    "pipeline_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also publish the pipeline as a RESTful API Endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish('training-pipeline')\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to continously publish a new pipelines, but have it published as the same URL as the version prior? For this, we can use [`PipelineEndpoint`](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelineendpoint?view=azure-ml-py), which keeps multiple `PublishedPipeline`s behind a single endpoint URL. It allows to set `default_version`, which determines to which `PublishedPipeline` it should route the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "endpoint_name = \"training-pipeline-endpoint\"\n",
    "\n",
    "try:\n",
    "   pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=endpoint_name)\n",
    "   # Add new default endpoint - only works from PublishedPipeline\n",
    "   pipeline_endpoint.add_default(published_pipeline)\n",
    "except Exception:\n",
    "    pipeline_endpoint = PipelineEndpoint.publish(workspace=ws,\n",
    "                                            name=endpoint_name,\n",
    "                                            pipeline=pipeline,\n",
    "                                            description=\"New Training Pipeline Endpoint\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c289faa3097d4ac9289519def538503f3010d283412eb21807c4abc0fc245ea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('azureml11': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}